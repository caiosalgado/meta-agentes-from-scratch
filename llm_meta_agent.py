#!/usr/bin/env python3
"""
LLM Meta Agent - Agente que cria outros agentes
Gera pipelines inteligentes de agentes LLM para maximizar performance
"""

import json
import time
import random
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import aisuite as ai
from llm_agent import LLM_Agent


class LLM_Meta_Agent:
    """
    Meta-agente que cria outros agentes LLM especializados.
    
    Aprende com hist√≥rico de performance e cria pipelines inteligentes
    combinando m√∫ltiplos agentes para maximizar acur√°cia.
    """
    
    # SYSTEM PROMPT √öNICO - SOURCE OF TRUTH
    SYSTEM_PROMPT = """Voc√™ √© um especialista em engenharia de agentes de IA.

RESPONDA EXCLUSIVAMENTE EM JSON com esta estrutura:
{
  "name": "Nome descritivo do pipeline",
  "pensamento": "Por que esta arquitetura vai funcionar melhor...", 
  "code": "C√≥digo Python execut√°vel que cria e orquestra os agentes"
}"""

    def __init__(self, 
                 model: str = "ollama:qwen3:32b",
                 history_file: str = "agent_history.json",
                 temperatura: float = 0.3):
        """
        Inicializa o Meta-Agente que cria outros agentes.
        
        Args:
            model: Modelo LLM para o meta-agente
            history_file: Arquivo para persistir hist√≥rico de agentes
            temperatura: Temperatura para gera√ß√£o de novos agentes
        """
        self.model = model
        self.temperatura = temperatura
        self.history_file = Path(history_file)
        self.agent_history = self._load_history()
        
        # Modelos dispon√≠veis para cria√ß√£o de agentes
        self.available_models = [
            "ollama:phi4:latest", 
            "ollama:gemma3:4b", 
            "ollama:gemma3:12b", 
            "ollama:gemma3:27b", 
            "ollama:qwen3:4b", 
            "ollama:qwen3:14b", 
            "ollama:qwen3:30b", 
            "ollama:qwen3:32b"
        ]
        
        # Cliente AI para o meta-agente
        self.client = ai.Client()
        self.client.configure({
            "ollama": {"timeout": 600}
        })
        
        # Arquitetura de resposta do meta-agente
        self.arquitetura_resposta = {
            "name": "Nome descritivo do agente/pipeline",
            "pensamento": "Racioc√≠nio sobre por que esta arquitetura vai funcionar",
            "code": "C√≥digo Python execut√°vel para criar e executar o(s) agente(s)"
        }
    
    def _load_history(self) -> List[Dict[str, Any]]:
        """Carrega hist√≥rico de agentes do arquivo JSON."""
        if self.history_file.exists():
            try:
                with open(self.history_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"Erro ao carregar hist√≥rico: {e}")
                return []
        return []
    
    def _save_history(self):
        """Salva hist√≥rico de agentes no arquivo JSON."""
        try:
            with open(self.history_file, 'w', encoding='utf-8') as f:
                json.dump(self.agent_history, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"Erro ao salvar hist√≥rico: {e}")
    
    def _generate_agent_id(self) -> str:
        """Gera ID √∫nico para novo agente."""
        if not self.agent_history:
            return "001"
        
        last_id = max([int(agent.get("agent_id", "0")) for agent in self.agent_history])
        return f"{last_id + 1:03d}"
    
    def get_functional_examples(self, top_n: int = 3) -> str:
        """
        Retorna exemplos de agentes funcionais (top 3 por acur√°cia + 1 aleat√≥rio).
        Em caso de empate na acur√°cia, usa avg_execution_time como crit√©rio.
        """
        if not self.agent_history:
            return "Nenhum hist√≥rico de agentes funcionais dispon√≠vel."
        
        # Filtrar agentes com performance v√°lida
        functional_agents = [
            agent for agent in self.agent_history 
            if agent.get("performance", {}).get("accuracy", 0) > 0
        ]
        
        if not functional_agents:
            return "Nenhum agente funcional encontrado no hist√≥rico."
        
        # Ordenar por acur√°cia (desc) e tempo de execu√ß√£o (asc)
        functional_agents.sort(
            key=lambda x: (
                -x.get("performance", {}).get("accuracy", 0),
                x.get("performance", {}).get("avg_execution_time", float('inf'))
            )
        )
        
        # Top N agentes
        top_agents = functional_agents[:top_n]
        
        # Adicionar 1 agente aleat√≥rio (se houver mais)
        remaining_agents = functional_agents[top_n:]
        if remaining_agents:
            random_agent = random.choice(remaining_agents)
            top_agents.append(random_agent)
        
        # Formatar exemplos
        examples = []
        for agent in top_agents:
            example = f"""
## {agent['name']} (ID: {agent['agent_id']})
**Performance:** Acur√°cia: {agent['performance']['accuracy']:.1f}%, Tempo: {agent['performance']['avg_execution_time']:.2f}s
**Pensamento:** {agent['thinking']}
**Configura√ß√£o:** {json.dumps(agent['config'], indent=2, ensure_ascii=False)}
"""
            examples.append(example)
        
        return "\n".join(examples)
    
    def get_non_functional_examples(self, max_n: int = 5) -> str:
        """
        Retorna exemplos de agentes n√£o funcionais para evitar padr√µes problem√°ticos.
        """
        if not self.agent_history:
            return "Nenhum hist√≥rico dispon√≠vel."
        
        # Filtrar agentes com baixa performance (< 30% acur√°cia)
        non_functional_agents = [
            agent for agent in self.agent_history 
            if agent.get("performance", {}).get("accuracy", 0) < 30
        ]
        
        if not non_functional_agents:
            return "Nenhum exemplo de baixa performance encontrado."
        
        # Pegar at√© max_n exemplos
        selected_agents = non_functional_agents[:max_n]
        
        # Formatar exemplos com tipos de erro comuns
        examples = []
        for agent in selected_agents:
            error_type = self._identify_error_type(agent)
            example = f"""
## {agent['name']} (ID: {agent['agent_id']}) - PROBLEM√ÅTICO
**Performance:** Acur√°cia: {agent['performance']['accuracy']:.1f}%
**Tipo de Erro:** {error_type}
**Problema Identificado:** {agent['thinking']}
**Configura√ß√£o que N√ÉO funcionou:** {json.dumps(agent['config'], indent=2, ensure_ascii=False)}
"""
            examples.append(example)
        
        return "\n".join(examples)
    
    def _identify_error_type(self, agent: Dict[str, Any]) -> str:
        """Identifica tipo de erro baseado na performance do agente."""
        accuracy = agent.get("performance", {}).get("accuracy", 0)
        execution_time = agent.get("performance", {}).get("avg_execution_time", 0)
        
        if accuracy == 0:
            return "Erro cr√≠tico - nenhuma resposta correta"
        elif accuracy < 10:
            return "Erro de l√≥gica - respostas predominantemente incorretas"
        elif execution_time > 10:
            return "Erro de performance - tempo de execu√ß√£o excessivo"
        elif accuracy < 30:
            return "Erro de arquitetura - baixa acur√°cia geral"
        else:
            return "Erro n√£o classificado"
    
    def _build_meta_prompt(self, task_explicacao: str) -> str:
        """Constr√≥i prompt para o meta-agente."""
        arquiteturas_agentes = f"Modelos dispon√≠veis: {', '.join(self.available_models)}"
        exemplos_funcionais = self.get_functional_examples()
        exemplos_nao_funcionais = self.get_non_functional_examples()
        
        prompt = f"""Seu objetivo √© criar PIPELINES INTELIGENTES de agentes que MAXIMIZEM A ACUR√ÅCIA e MINIMIZEM O TEMPO DE EXECU√á√ÉO atrav√©s de colabora√ß√£o, especializa√ß√£o e racioc√≠nio distribu√≠do.

Para melhorar a performance, voc√™ pode usar as seguintes t√©cnicas:
- Usar diferentes modelos para diferentes fun√ß√µes
- Pipelines podem usar outputs anteriores via arquitetura_respostas_anteriores
- Pense em especializa√ß√£o: cada agente tem fun√ß√£o espec√≠fica
- Considere edge cases e robustez

Para minimizar o tempo de execu√ß√£o, voc√™ pode usar as seguintes t√©cnicas:
- Usar modelos mais leves
- Usar pipelines mais simples
- Usar pipelines com menos agentes
- Usar pipelines com menos itera√ß√µes

T√âCNICAS DISPON√çVEIS:
- Single Agent: Um agente especializado
- Sequential Pipeline: Agente1 ‚Üí Agente2 ‚Üí ... ‚Üí AgentN  
- Debate Pipeline: M√∫ltiplos especialistas ‚Üí Decis√£o final
- Reflection Pipeline: Agente principal ‚Üî Agente revisor
- Hierarchical: Agente coordenador + agentes especializados

MODELOS DISPON√çVEIS:
{', '.join(self.available_models)}

# Hist√≥rico de Arquiteturas
{arquiteturas_agentes}

# Exemplos de ALTA Performance (FUNCIONAIS)
{exemplos_funcionais}

# Exemplos de BAIXA Performance (EVITAR)
{exemplos_nao_funcionais}

# Sua Miss√£o
{task_explicacao}

RESPONDA EXCLUSIVAMENTE EM JSON com esta estrutura:
{{
  "name": "Nome descritivo do pipeline",
  "pensamento": "Por que esta arquitetura vai funcionar melhor...",
  "code": "C√≥digo Python execut√°vel que cria e orquestra os agentes"
}}

DIRETRIZES IMPORTANTES:
- Use diferentes modelos para diferentes fun√ß√µes
- Varie temperaturas: baixa (0.1-0.3) para precis√£o, media (0.4-0.6) para normalidade e alta (0.7-0.9) para criatividade
- Pipelines podem usar outputs anteriores via arquitetura_respostas_anteriores
- Pense em especializa√ß√£o: cada agente tem fun√ß√£o espec√≠fica
- Considere edge cases e robustez
- O c√≥digo deve retornar um dicion√°rio final com a resposta

IMPORTANTE: O c√≥digo deve implementar uma fun√ß√£o solve_problem(problem_data) que:
1. Recebe problem_data (dicion√°rio do problema LeetCode)
2. Executa o pipeline de agentes
3. Retorna dicion√°rio com a resposta final

CRIE UM PIPELINE REVOLUCION√ÅRIO!"""
        
        return prompt
    
    def generate_agent_code(self, task_explicacao: str) -> Dict[str, Any]:
        """
        Gera c√≥digo execut√°vel para novo(s) agente(s).
        
        Args:
            task_explicacao: Descri√ß√£o da tarefa/objetivo do agente
            
        Returns:
            Dict com name, pensamento e code do agente gerado
        """
        prompt = self._build_meta_prompt(task_explicacao)
        
        messages = [
            {"role": "system", "content": self.SYSTEM_PROMPT},
            {"role": "user", "content": prompt}
        ]
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperatura
            )
            
            raw_response = response.choices[0].message.content
            
            # Parse JSON da resposta
            try:
                # Tentar parse direto
                result = json.loads(raw_response)
            except json.JSONDecodeError:
                # Extrair JSON se estiver em markdown
                start_idx = raw_response.find('{')
                end_idx = raw_response.rfind('}')
                if start_idx != -1 and end_idx != -1:
                    json_str = raw_response[start_idx:end_idx + 1]
                    result = json.loads(json_str)
                else:
                    return {
                        "name": "Erro de Parse",
                        "pensamento": "Erro ao fazer parse da resposta JSON",
                        "code": f"# Erro: resposta n√£o √© JSON v√°lido\n# {raw_response}"
                    }
            
            return result
            
        except Exception as e:
            return {
                "name": "Erro de Gera√ß√£o",
                "pensamento": f"Erro ao gerar agente: {str(e)}",
                "code": f"# Erro na gera√ß√£o: {str(e)}"
            }
    
    def test_pipeline_multiple_times(self, pipeline_code: str, runs: int = 10) -> Dict[str, Any]:
        """
        Executa pipeline m√∫ltiplas vezes e retorna estat√≠sticas consolidadas.
        
        Args:
            pipeline_code: C√≥digo Python do pipeline
            runs: N√∫mero de execu√ß√µes para teste
            
        Returns:
            Dict com accuracy m√©dia e avg_execution_time
        """
        # Carregar problemas LeetCode
        try:
            with open("leetcode_problems.json", encoding="utf-8") as f:
                problems = json.load(f)["problems"]
        except Exception as e:
            return {
                "accuracy": 0.0,
                "avg_execution_time": 0.0,
                "error": f"Erro ao carregar problemas: {str(e)}"
            }
        
        all_accuracies = []
        all_execution_times = []
        successful_runs = 0
        
        for run in range(runs):
            print(f"Executando teste {run + 1}/{runs}...")
            
            try:
                # Executar c√≥digo do pipeline
                local_vars = {}
                exec(pipeline_code, globals(), local_vars)
                
                if 'solve_problem' not in local_vars:
                    print(f"Run {run + 1}: Fun√ß√£o solve_problem n√£o encontrada")
                    continue
                
                solve_function = local_vars['solve_problem']
                
                # Testar cada problema
                correct_count = 0
                total_problems = len(problems)
                total_time = 0
                
                for problem in problems:
                    start_time = time.time()
                    
                    try:
                        # Executar fun√ß√£o do pipeline
                        result = solve_function(problem)
                        execution_time = time.time() - start_time
                        total_time += execution_time
                        
                        # Verificar se resultado √© v√°lido
                        if isinstance(result, dict) and 'code' in result:
                            # Testar c√≥digo gerado
                            from llm_agent import LLM_Agent
                            temp_agent = LLM_Agent(
                                role="Test Agent",
                                instruction="Test",
                                arquitetura_resposta={"code": "code"}
                            )
                            
                            accuracy, _ = temp_agent.test_code_accuracy(
                                result['code'], 
                                problem.get('tests', [])
                            )
                            
                            if accuracy > 50:  # Considera correto se > 50% dos testes passaram
                                correct_count += 1
                        
                    except Exception as e:
                        print(f"Erro no problema {problem['id']}: {str(e)}")
                        total_time += 1.0  # Penalty time
                
                # Calcular m√©tricas do run
                run_accuracy = (correct_count / total_problems) * 100 if total_problems > 0 else 0
                run_avg_time = total_time / total_problems if total_problems > 0 else 0
                
                all_accuracies.append(run_accuracy)
                all_execution_times.append(run_avg_time)
                successful_runs += 1
                
                print(f"Run {run + 1}: Acur√°cia = {run_accuracy:.1f}%, Tempo = {run_avg_time:.2f}s")
                
            except Exception as e:
                print(f"Erro no run {run + 1}: {str(e)}")
                all_accuracies.append(0.0)
                all_execution_times.append(10.0)  # Penalty time
        
        # Calcular estat√≠sticas finais
        if successful_runs > 0:
            final_accuracy = sum(all_accuracies) / len(all_accuracies)
            final_avg_time = sum(all_execution_times) / len(all_execution_times)
        else:
            final_accuracy = 0.0
            final_avg_time = 0.0
        
        return {
            "accuracy": final_accuracy,
            "avg_execution_time": final_avg_time,
            "successful_runs": successful_runs,
            "total_runs": runs,
            "all_accuracies": all_accuracies,
            "all_execution_times": all_execution_times
        }
    
    def create_and_evaluate_agent(self, task_explicacao: str) -> Dict[str, Any]:
        """
        Fluxo completo: gera agente ‚Üí testa 10x ‚Üí salva no hist√≥rico.
        
        Args:
            task_explicacao: Descri√ß√£o da tarefa para o novo agente
            
        Returns:
            Dict com informa√ß√µes completas do agente criado e testado
        """
        print("ü§ñ Gerando novo agente...")
        
        # Gerar c√≥digo do agente
        agent_spec = self.generate_agent_code(task_explicacao)
        
        if "Erro" in agent_spec.get("name", ""):
            return {
                "success": False,
                "error": "Erro na gera√ß√£o do agente",
                "details": agent_spec
            }
        
        print(f"‚úÖ Agente gerado: {agent_spec['name']}")
        print(f"üí≠ Pensamento: {agent_spec['pensamento']}")
        
        # Testar agente m√∫ltiplas vezes
        print("üß™ Testando agente 10 vezes...")
        performance = self.test_pipeline_multiple_times(agent_spec['code'], runs=10)
        
        # Criar entrada do hist√≥rico
        agent_entry = {
            "agent_id": self._generate_agent_id(),
            "name": agent_spec['name'],
            "creation_timestamp": datetime.now().isoformat(),
            "config": {
                "type": "pipeline",  # Detectar automaticamente ou assumir pipeline
                "code": agent_spec['code']
            },
            "performance": {
                "accuracy": performance['accuracy'],
                "avg_execution_time": performance['avg_execution_time']
            },
            "thinking": agent_spec['pensamento'],
            "task_explanation": task_explicacao,
            "test_details": performance
        }
        
        # Adicionar ao hist√≥rico
        self.agent_history.append(agent_entry)
        self._save_history()
        
        print(f"üìä Performance: {performance['accuracy']:.1f}% acur√°cia, {performance['avg_execution_time']:.2f}s tempo m√©dio")
        print(f"üíæ Salvo no hist√≥rico com ID: {agent_entry['agent_id']}")
        
        return {
            "success": True,
            "agent": agent_entry,
            "performance": performance
        }
    
    def get_agent_statistics(self) -> Dict[str, Any]:
        """Retorna estat√≠sticas do hist√≥rico de agentes."""
        if not self.agent_history:
            return {"total_agents": 0}
        
        accuracies = [agent.get("performance", {}).get("accuracy", 0) for agent in self.agent_history]
        execution_times = [agent.get("performance", {}).get("avg_execution_time", 0) for agent in self.agent_history]
        
        return {
            "total_agents": len(self.agent_history),
            "avg_accuracy": sum(accuracies) / len(accuracies),
            "max_accuracy": max(accuracies),
            "min_accuracy": min(accuracies),
            "avg_execution_time": sum(execution_times) / len(execution_times),
            "functional_agents": len([a for a in self.agent_history if a.get("performance", {}).get("accuracy", 0) >= 30]),
            "high_performance_agents": len([a for a in self.agent_history if a.get("performance", {}).get("accuracy", 0) >= 70])
        }
    
    def list_top_agents(self, top_n: int = 5) -> List[Dict[str, Any]]:
        """Lista os top N agentes por performance."""
        if not self.agent_history:
            return []
        
        sorted_agents = sorted(
            self.agent_history,
            key=lambda x: (
                -x.get("performance", {}).get("accuracy", 0),
                x.get("performance", {}).get("avg_execution_time", float('inf'))
            )
        )
        
        return sorted_agents[:top_n]


# Exemplo de uso
if __name__ == "__main__":
    # Criar meta-agente
    meta_agent = LLM_Meta_Agent(model="ollama:qwen3:32b", temperatura=0.3)
    
    # Exemplo de cria√ß√£o de agente
    task = """
    Criar um agente/pipeline que maximize a acur√°cia em problemas de algoritmos LeetCode.
    O agente deve ser capaz de resolver problemas de diferentes categorias como:
    - Problemas de arrays e listas
    - Problemas de strings e manipula√ß√£o de texto  
    - Problemas de matem√°tica e l√≥gica
    - Problemas de estruturas de dados b√°sicas
    
    Foque em crear uma solu√ß√£o robusta que combine diferentes t√©cnicas de racioc√≠nio.
    """
    
    print("üöÄ Iniciando cria√ß√£o de novo agente...")
    result = meta_agent.create_and_evaluate_agent(task)
    
    if result["success"]:
        print("\n‚úÖ Agente criado e testado com sucesso!")
        print(f"üìä Estat√≠sticas: {meta_agent.get_agent_statistics()}")
    else:
        print(f"\n‚ùå Erro: {result['error']}") 